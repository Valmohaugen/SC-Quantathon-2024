{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.qbraid/environments/qiskit_9vrlwn/pyenv/lib/python3.11/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in ./.qbraid/environments/qiskit_9vrlwn/pyenv/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.qbraid/environments/qiskit_9vrlwn/pyenv/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.qbraid/environments/qiskit_9vrlwn/pyenv/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.qbraid/environments/qiskit_9vrlwn/pyenv/lib/python3.11/site-packages (from scikit-learn) (3.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in ./.qbraid/environments/qiskit_9vrlwn/pyenv/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in ./.qbraid/environments/qiskit_9vrlwn/pyenv/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./.qbraid/environments/qiskit_9vrlwn/pyenv/lib/python3.11/site-packages (from xgboost) (2.23.4)\n",
      "Requirement already satisfied: scipy in ./.qbraid/environments/qiskit_9vrlwn/pyenv/lib/python3.11/site-packages (from xgboost) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries (install in your environment first)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log2, sqrt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from your datafile or the provided datafile in this folder to classify QRNG data. \n",
    "# Suggested classification strategies include QRNG vs PRNG, QPU vs Simulator, or by individual QPU\n",
    "\n",
    "#Sample datafile of QRNG (IBM QPUs) vs PRNG data, 12k lines each. Label 1 is QRNG, label 2 is PRNG\n",
    "data_filePath = 'QRNGvsPRNG_TrainingData.txt'\n",
    "\n",
    "# Read data file, make dataframe, process labels, and combine/concatenate individual input lines into\n",
    "# larger input lines to create training and testing datasets to input into gradient boosting model.\n",
    "# Hint: use train_test_split method from sklearn\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(data_filePath, sep=\" \", names=['bitstrings', 'Quantum or Classical'])\n",
    "\n",
    "# Convert 'Quantum or Classical' labels to numeric values (e.g., 1 for QRNG, 2 for PRNG)\n",
    "# df['Quantum or Classical'] = df['Quantum or Classical'].apply(lambda x: 1 if x == 'QRNG' else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame (assuming you already have it)\n",
    "# df['bitstrings'] contains the bitstrings of length 100\n",
    "# df = pd.DataFrame({'bitstrings': ['1100100100001111110110...', '1010101010101010101010...', ...]})\n",
    "\n",
    "# Function to calculate Shannon entropy\n",
    "def shannon_entropy(bitstring):\n",
    "    # Convert bitstring to a numpy array of integers\n",
    "    bits = np.array([int(bit) for bit in bitstring])\n",
    "    \n",
    "    # Calculate the frequency of 0's and 1's\n",
    "    counts = np.bincount(bits)\n",
    "    probabilities = counts / len(bits)\n",
    "    \n",
    "    # Filter out zero probabilities to avoid log2(0)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Apply the entropy function to each bitstring in the DataFrame\n",
    "df['entropy'] = df['bitstrings'].apply(shannon_entropy)\n",
    "\n",
    "# Now the DataFrame 'df' contains an additional 'entropy' column\n",
    "#print(df[['bitstrings', 'entropy']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              bitstrings  chi_sq  \\\n",
      "0      0000101000100111111110011011110111101101010111...    7.84   \n",
      "1      0100101111010000110010000101001110101001001010...    0.04   \n",
      "2      1000101010100100011100101111011111001110011101...    1.96   \n",
      "3      0111101100010110010000011111111001110001100110...    0.36   \n",
      "4      1111100000011110111111111111101001100100011010...    2.56   \n",
      "...                                                  ...     ...   \n",
      "23995  0000100111011010101101011101001111011001011001...    0.36   \n",
      "23996  1100111110000000100110000101100100111101010011...    0.04   \n",
      "23997  1001100110110111101011011000101010100111111011...    7.84   \n",
      "23998  0000010010101011000101010110101000100101001001...    0.64   \n",
      "23999  1010101001100100100101100111100101010011001010...    0.64   \n",
      "\n",
      "       chi_sq_p_value  \n",
      "0            0.005110  \n",
      "1            0.841481  \n",
      "2            0.161513  \n",
      "3            0.548506  \n",
      "4            0.109599  \n",
      "...               ...  \n",
      "23995        0.548506  \n",
      "23996        0.841481  \n",
      "23997        0.005110  \n",
      "23998        0.423711  \n",
      "23999        0.423711  \n",
      "\n",
      "[24000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2\n",
    "def chi_squared_test(bits):\n",
    "        counts = np.bincount(list(bits), minlength=2)\n",
    "        observed = counts\n",
    "        expected = np.array([len(bits)/2, len(bits)/2])\n",
    "        chi_sq = np.sum((observed - expected) ** 2 / expected)\n",
    "        # Degrees of freedom = number of categories - 1 = 1\n",
    "        p_value = 1 - chi2.cdf(chi_sq, df=1)\n",
    "        return chi_sq, p_value\n",
    "\n",
    "# Apply the function and store results in two new columns\n",
    "df[['chi_sq', 'chi_sq_p_value']] = pd.DataFrame(df['bitstrings'].apply(chi_squared_test).tolist(), index=df.index)\n",
    "\n",
    "# View the updated DataFrame\n",
    "print(df[['bitstrings', 'chi_sq', 'chi_sq_p_value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              bitstrings  freq_mag_1  \\\n",
      "0      0000101000100111111110011011110111101101010111...   28.000000   \n",
      "1      0100101111010000110010000101001110101001001010...   17.189026   \n",
      "2      1000101010100100011100101111011111001110011101...   21.908773   \n",
      "3      0111101100010110010000011111111001110001100110...   22.221626   \n",
      "4      1111100000011110111111111111101001100100011010...   21.651404   \n",
      "...                                                  ...         ...   \n",
      "23995  0000100111011010101101011101001111011001011001...   21.404946   \n",
      "23996  1100111110000000100110000101100100111101010011...   18.405510   \n",
      "23997  1001100110110111101011011000101010100111111011...   28.000000   \n",
      "23998  0000010010101011000101010110101000100101001001...   22.435155   \n",
      "23999  1010101001100100100101100111100101010011001010...   23.954548   \n",
      "\n",
      "       freq_mag_2  freq_mag_3  freq_mag_4  freq_mag_5  \n",
      "0       19.921323   16.173097   15.578477   15.174516  \n",
      "1       16.956262   16.124515   15.853050   15.719933  \n",
      "2       19.569520   16.367229   16.146352   16.145532  \n",
      "3       21.050439   19.401910   18.867962   18.621162  \n",
      "4       16.914346   16.842395   16.630112   16.007237  \n",
      "...           ...         ...         ...         ...  \n",
      "23995   21.223814   18.677581   16.863668   16.433162  \n",
      "23996   16.989297   15.005964   14.321939   14.101813  \n",
      "23997   19.404923   18.374018   17.992048   15.608276  \n",
      "23998   17.366073   16.973227   14.994154   14.178604  \n",
      "23999   17.279708   16.757210   15.847279   15.014529  \n",
      "\n",
      "[24000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "# Function to get the top 5 dominant magnitudes of frequencies\n",
    "def dominant_frequencies(bitstring, top_n=5):\n",
    "    # Convert bitstring to list of integers (0s and 1s)\n",
    "    bits = np.array([int(bit) for bit in bitstring])\n",
    "    n = len(bits)  # Length of the bitstring\n",
    "\n",
    "    # Convert bits to -1 and 1 for FFT\n",
    "    signal = 2 * bits - 1\n",
    "    fft_result = fft(signal)\n",
    "    \n",
    "    # Only take the positive frequencies\n",
    "    freqs = np.fft.fftfreq(n)\n",
    "    magnitudes = np.abs(fft_result)\n",
    "    positive_freqs = freqs[:n//2]\n",
    "    positive_magnitudes = magnitudes[:n//2]\n",
    "    \n",
    "    # Find the top_n dominant frequencies\n",
    "    indices = np.argsort(positive_magnitudes)[-top_n:]\n",
    "    dominant_mags = positive_magnitudes[indices]\n",
    "    \n",
    "    # Sort the dominant magnitudes in descending order\n",
    "    sorted_mags = np.sort(dominant_mags)[::-1]\n",
    "    \n",
    "    # If fewer than top_n magnitudes, pad with NaNs\n",
    "    if len(sorted_mags) < top_n:\n",
    "        sorted_mags = np.pad(sorted_mags, (0, top_n - len(sorted_mags)), constant_values=np.nan)\n",
    "    \n",
    "    return sorted_mags\n",
    "\n",
    "# Assuming df['bitstrings'] contains the bitstrings of length 100\n",
    "# Apply the function and expand it into separate columns\n",
    "df[['freq_mag_1', 'freq_mag_2', 'freq_mag_3', 'freq_mag_4', 'freq_mag_5']] = pd.DataFrame(\n",
    "    df['bitstrings'].apply(dominant_frequencies).tolist(), index=df.index\n",
    ")\n",
    "\n",
    "# View the updated DataFrame with the new columns\n",
    "print(df[['bitstrings', 'freq_mag_1', 'freq_mag_2', 'freq_mag_3', 'freq_mag_4', 'freq_mag_5']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(bitstring, lag=1):\n",
    "    # Convert the bitstring to a numpy array of integers (0 and 1)\n",
    "    bits = np.array([int(bit) for bit in bitstring])\n",
    "    \n",
    "    if lag >= len(bits):\n",
    "        raise ValueError(\"Lag is too large for the bitstream length.\")\n",
    "    \n",
    "    # Shift the bits by the given lag\n",
    "    shifted = np.roll(bits, -lag)\n",
    "    \n",
    "    # Calculate correlation excluding the wrapped-around elements\n",
    "    valid_length = len(bits) - lag\n",
    "    correlation = np.corrcoef(bits[:valid_length], shifted[:valid_length])[0, 1]\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "df['autocorrelation_lag1'] = df['bitstrings'].apply(autocorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sp80022suite\n",
      "  Using cached sp80022suite-0.0.8.tar.gz (1.0 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sp80022suite\n",
      "  Building wheel for sp80022suite (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sp80022suite: filename=sp80022suite-0.0.8-cp311-cp311-linux_x86_64.whl size=1052852 sha256=a0c8708e21851811a6505cc8da6bd352d10a3f7c82d798badff55ded75ee425f\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/28/5d/0b/fe2a1c641ced7cd3726ced2673263f0f3541f2f23ba6dc45ae\n",
      "Successfully built sp80022suite\n",
      "Installing collected packages: sp80022suite\n",
      "Successfully installed sp80022suite-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install sp80022suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              bitstrings  freq_ones  \\\n",
      "0      0000101000100111111110011011110111101101010111...         64   \n",
      "1      0100101111010000110010000101001110101001001010...         49   \n",
      "2      1000101010100100011100101111011111001110011101...         57   \n",
      "3      0111101100010110010000011111111001110001100110...         53   \n",
      "4      1111100000011110111111111111101001100100011010...         58   \n",
      "...                                                  ...        ...   \n",
      "23995  0000100111011010101101011101001111011001011001...         47   \n",
      "23996  1100111110000000100110000101100100111101010011...         49   \n",
      "23997  1001100110110111101011011000101010100111111011...         64   \n",
      "23998  0000010010101011000101010110101000100101001001...         46   \n",
      "23999  1010101001100100100101100111100101010011001010...         46   \n",
      "\n",
      "       freq_zeros  block_freqs_max  block_freqs_mean  block_freq_chi_squared  \n",
      "0              36              9.0               6.4                    2.00  \n",
      "1              51              7.0               4.9                    0.76  \n",
      "2              43              9.0               5.7                    1.16  \n",
      "3              47              7.0               5.3                    0.36  \n",
      "4              42              9.0               5.8                    1.92  \n",
      "...           ...              ...               ...                     ...  \n",
      "23995          53              7.0               4.7                    0.84  \n",
      "23996          51              7.0               4.9                    1.16  \n",
      "23997          36              9.0               6.4                    1.68  \n",
      "23998          54              8.0               4.6                    1.12  \n",
      "23999          54              6.0               4.6                    0.48  \n",
      "\n",
      "[24000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "def frequency_test(bitstring):\n",
    "    \"\"\"\n",
    "    Perform the Frequency Test on a bitstring.\n",
    "    \n",
    "    :param bitstring: A 1D numpy array or list of bits (0s and 1s).\n",
    "    :return: A tuple containing the number of 1s, number of 0s, and the test statistic.\n",
    "    \"\"\"\n",
    "    count_ones = list(bitstring).count('1')\n",
    "    count_zeros = len(bitstring) - count_ones\n",
    "    \n",
    "    return count_ones, count_zeros\n",
    "\n",
    "def block_frequency_test(bitstring, M):\n",
    "    \"\"\"\n",
    "    Perform the Block Frequency Test on a bitstring.\n",
    "    \n",
    "    :param bitstring: A 1D numpy array or list of bits (0s and 1s).\n",
    "    :param M: Block size for the block frequency test.\n",
    "    :return: The frequencies of 1s in each block and the test statistic.\n",
    "    \"\"\"\n",
    "    bitstring = [int(c) for c in bitstring]\n",
    "    n = len(bitstring)\n",
    "    num_blocks = n // M\n",
    "    block_frequencies = np.zeros(num_blocks)\n",
    "\n",
    "    # Compute block frequencies\n",
    "    for i in range(num_blocks):\n",
    "        block = bitstring[i * M:(i + 1) * M]\n",
    "        block_frequencies[i] = np.sum(block)\n",
    "\n",
    "    # Calculate the mean and variance of the block frequencies\n",
    "    mean_frequency = np.mean(block_frequencies)\n",
    "    variance_frequency = np.var(block_frequencies)\n",
    "\n",
    "    # The expected mean and variance for a random sequence\n",
    "    expected_mean = M / 2\n",
    "    expected_variance = M / 4\n",
    "\n",
    "    # Chi-squared statistic\n",
    "    chi_squared = ((mean_frequency - expected_mean) ** 2 / expected_variance) + \\\n",
    "                  (variance_frequency / expected_variance)\n",
    "\n",
    "    return block_frequencies.max(), block_frequencies.mean(), chi_squared\n",
    "\n",
    "df['freq_ones'], df['freq_zeros'] = zip(*df['bitstrings'].apply(frequency_test))\n",
    "\n",
    "# Set block size for Block Frequency Test\n",
    "M = 10  # Example block size\n",
    "df['block_freqs_max'], df['block_freqs_mean'], df['block_freq_chi_squared'] = zip(*df['bitstrings'].apply(lambda x: block_frequency_test(x, M)))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df[['bitstrings', 'freq_ones', 'freq_zeros', 'block_freqs_max', 'block_freqs_mean', 'block_freq_chi_squared']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       runs  runs_p_value\n",
      "0        46      0.757941\n",
      "1        52      0.308650\n",
      "2        51      0.382177\n",
      "3        49      0.539795\n",
      "4        42      0.933395\n",
      "...     ...           ...\n",
      "23995    55      0.135627\n",
      "23996    49      0.539795\n",
      "23997    49      0.539795\n",
      "23998    59      0.028444\n",
      "23999    63      0.003319\n",
      "\n",
      "[24000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "def runs_test(bits):\n",
    "    \"\"\"\n",
    "    Performs the Runs Test on a binary sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    bits (array-like): A binary sequence (1s and 0s).\n",
    "    \n",
    "    Returns:\n",
    "    (int, float): The number of runs and the p-value of the test.\n",
    "    \"\"\"\n",
    "    # Convert bits to a numpy array\n",
    "    bits = np.array([int(c) for c in bits])\n",
    "    \n",
    "    # Count the number of runs\n",
    "    runs = 1  # Start with the first run\n",
    "    for i in range(1, len(bits)):\n",
    "        if bits[i] != bits[i-1]:\n",
    "            runs += 1\n",
    "            \n",
    "    n1 = np.sum(bits)  # Number of 1s\n",
    "    n0 = len(bits) - n1  # Number of 0s\n",
    "    \n",
    "    # Calculate the expected number of runs and variance\n",
    "    expected_runs = (2 * n1 * n0) / (n1 + n0) + 1\n",
    "    variance_runs = (2 * n1 * n0 * (2 * n1 * n0 - n1 - n0)) / ((n1 + n0) ** 2 * (n1 + n0 - 1))\n",
    "    \n",
    "    # Z-score\n",
    "    z = (runs - expected_runs) / np.sqrt(variance_runs)\n",
    "    \n",
    "    # Calculate p-value from z-score\n",
    "    p_value = 1 - binom.cdf(runs, n=n1 + n0, p=0.5)  # Note: binom.cdf is not directly usable for z-scores\n",
    "    \n",
    "    return runs, p_value\n",
    "\n",
    "df['runs'], df['runs_p_value'] = zip(*df['bitstrings'].apply(runs_test))\n",
    "print(df[['runs', 'runs_p_value']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        50\n",
      "1        50\n",
      "2        51\n",
      "3        51\n",
      "4        51\n",
      "         ..\n",
      "23995    50\n",
      "23996    49\n",
      "23997    50\n",
      "23998    52\n",
      "23999    50\n",
      "Name: linear_complexity, Length: 24000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def xor(a, b):\n",
    "    \"\"\"\n",
    "    Returns the XOR of two binary values (0 or 1).\n",
    "    \n",
    "    Parameters:\n",
    "    a (int): First binary value (0 or 1).\n",
    "    b (int): Second binary value (0 or 1).\n",
    "    \n",
    "    Returns:\n",
    "    int: Result of a XOR b (0 or 1).\n",
    "    \"\"\"\n",
    "    return (a or b) and not (a and b)\n",
    "\n",
    "def linear_complexity(bits):\n",
    "    \"\"\"\n",
    "    Computes the Linear Complexity of a binary sequence using the Berlekamp-Massey algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    bits (array-like): A binary sequence (1s and 0s).\n",
    "    \n",
    "    Returns:\n",
    "    int: The linear complexity of the sequence.\n",
    "    \"\"\"\n",
    "    bits = np.array([int(c) for c in bits])\n",
    "    n = len(bits)\n",
    "    l = 0  # Linear complexity\n",
    "    m = -1  # Previous index where the error occurred\n",
    "    C = np.zeros(n)\n",
    "    B = np.zeros(n)\n",
    "    C[0] = 1  # The polynomial is initialized with a leading coefficient of 1\n",
    "    B[0] = 1\n",
    "    \n",
    "    for n in range(n):\n",
    "        # Calculate the discrepancy\n",
    "        discrepancy = bits[n]\n",
    "        for i in range(1, l + 1):\n",
    "            discrepancy = xor(discrepancy, (C[i] * bits[n - i]))\n",
    "        \n",
    "        if discrepancy == 1:  # An error occurred\n",
    "            T = C.copy()\n",
    "            for i in range(n - m):\n",
    "                if m + i < n:\n",
    "                    C[m + i] = xor(C[m + i], B[i])\n",
    "            if l <= n // 2:\n",
    "                l = n + 1 - l\n",
    "                m = n\n",
    "                B = T.copy()\n",
    "    \n",
    "    return l\n",
    "\n",
    "df['linear_complexity'] = df['bitstrings'].apply(linear_complexity)\n",
    "print(df['linear_complexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum or Classical      1.000000\n",
      "entropy                   0.043439\n",
      "chi_sq                   -0.043405\n",
      "freq_zeros                0.038105\n",
      "freq_ones                -0.038105\n",
      "block_freqs_mean         -0.038105\n",
      "chi_sq_p_value            0.032646\n",
      "block_freqs_max          -0.022674\n",
      "block_freq_chi_squared   -0.021062\n",
      "runs_p_value             -0.005076\n",
      "runs                      0.004439\n",
      "linear_complexity         0.002260\n",
      "Name: Quantum or Classical, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Normalize a specific column in the DataFrame (let's assume 'column_name' is the one you want to normalize)\n",
    "def zscore_col(df, colname):\n",
    "    df[colname] = (df[colname] - df[colname].mean()) / df[colname].std()\n",
    "\n",
    "metrics = ['linear_complexity', 'runs_p_value', 'runs', 'chi_sq','chi_sq_p_value','entropy', 'freq_ones', 'freq_zeros', 'block_freqs_max', 'block_freqs_mean', 'block_freq_chi_squared']\n",
    "\n",
    "\n",
    "\n",
    "for colname in metrics:\n",
    "    zscore_col(df, colname)\n",
    "\n",
    "# Calculate correlation\n",
    "correlation_matrix = df[metrics + ['Quantum or Classical']].corr()\n",
    "correlation_with_target = correlation_matrix['Quantum or Classical'].sort_values(ascending=False, key=abs)\n",
    "print(correlation_with_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'strings' column into separate columns for each character\n",
    "df_split = df['bitstrings'].str.split('', expand=True)\n",
    "\n",
    "# Fill NaN values with an empty string (optional)\n",
    "df_split.fillna('', inplace=True)\n",
    "\n",
    "# Concatenate the split DataFrame with the original DataFrame\n",
    "df_combined = pd.concat([df, df_split], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7613944  -0.9206176   0.71592708 ...  0.65513071 -0.62348465\n",
      "   0.15281114]\n",
      " [-0.20699275 -0.23061921  0.11635803 ... -0.45314709 -0.23250347\n",
      "  -1.34266465]\n",
      " [ 0.7613944  -0.23061921  0.11635803 ... -0.45314709  0.1584777\n",
      "  -1.15573018]\n",
      " ...\n",
      " [-2.14376706  1.68955127 -1.6823491  ... -0.45314709 -0.42799406\n",
      "  -0.87532847]\n",
      " [ 2.69816871  0.31844351 -0.28335467 ...  0.65513071  0.54945888\n",
      "   0.15281114]\n",
      " [-1.17537991  0.84638844 -0.68306736 ... -0.45314709 -0.23250347\n",
      "   0.52668009]]\n",
      "[0 1 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Prepare input features (X) and target labels (y)\n",
    "columns_to_exclude = ['bitstrings', 'Quantum or Classical']\n",
    "df_data = df.drop(columns=columns_to_exclude)\n",
    "X = pd.DataFrame(df[metrics])\n",
    "X = np.array(X)\n",
    "y = np.array(df['Quantum or Classical'])\n",
    "y = y - 1\n",
    "# print(X)\n",
    "# print(sorted(y))\n",
    "# print(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.69239\n",
      "[1]\tvalidation_0-logloss:0.69274\n",
      "[2]\tvalidation_0-logloss:0.69296\n",
      "[3]\tvalidation_0-logloss:0.69305\n",
      "[4]\tvalidation_0-logloss:0.69373\n",
      "[5]\tvalidation_0-logloss:0.69354\n",
      "[6]\tvalidation_0-logloss:0.69383\n",
      "[7]\tvalidation_0-logloss:0.69423\n",
      "[8]\tvalidation_0-logloss:0.69480\n",
      "[9]\tvalidation_0-logloss:0.69484\n",
      "[10]\tvalidation_0-logloss:0.69494\n",
      "[11]\tvalidation_0-logloss:0.69504\n",
      "[12]\tvalidation_0-logloss:0.69548\n",
      "[13]\tvalidation_0-logloss:0.69594\n",
      "[14]\tvalidation_0-logloss:0.69618\n",
      "[15]\tvalidation_0-logloss:0.69633\n",
      "[16]\tvalidation_0-logloss:0.69698\n",
      "[17]\tvalidation_0-logloss:0.69712\n",
      "[18]\tvalidation_0-logloss:0.69734\n",
      "[19]\tvalidation_0-logloss:0.69772\n",
      "[20]\tvalidation_0-logloss:0.69740\n",
      "[21]\tvalidation_0-logloss:0.69772\n",
      "[22]\tvalidation_0-logloss:0.69795\n",
      "[23]\tvalidation_0-logloss:0.69845\n",
      "[24]\tvalidation_0-logloss:0.69869\n",
      "[25]\tvalidation_0-logloss:0.69876\n",
      "[26]\tvalidation_0-logloss:0.69847\n",
      "[27]\tvalidation_0-logloss:0.69871\n",
      "[28]\tvalidation_0-logloss:0.69906\n",
      "[29]\tvalidation_0-logloss:0.69926\n",
      "[30]\tvalidation_0-logloss:0.69944\n",
      "[31]\tvalidation_0-logloss:0.69964\n",
      "[32]\tvalidation_0-logloss:0.69987\n",
      "[33]\tvalidation_0-logloss:0.70029\n",
      "[34]\tvalidation_0-logloss:0.70044\n",
      "[35]\tvalidation_0-logloss:0.70059\n",
      "[36]\tvalidation_0-logloss:0.70113\n",
      "[37]\tvalidation_0-logloss:0.70114\n",
      "[38]\tvalidation_0-logloss:0.70137\n",
      "[39]\tvalidation_0-logloss:0.70158\n",
      "[40]\tvalidation_0-logloss:0.70175\n",
      "[41]\tvalidation_0-logloss:0.70163\n",
      "[42]\tvalidation_0-logloss:0.70218\n",
      "[43]\tvalidation_0-logloss:0.70285\n",
      "[44]\tvalidation_0-logloss:0.70295\n",
      "[45]\tvalidation_0-logloss:0.70351\n",
      "[46]\tvalidation_0-logloss:0.70423\n",
      "[47]\tvalidation_0-logloss:0.70473\n",
      "[48]\tvalidation_0-logloss:0.70530\n",
      "[49]\tvalidation_0-logloss:0.70540\n",
      "[50]\tvalidation_0-logloss:0.70527\n",
      "[51]\tvalidation_0-logloss:0.70543\n",
      "[52]\tvalidation_0-logloss:0.70566\n",
      "[53]\tvalidation_0-logloss:0.70615\n",
      "[54]\tvalidation_0-logloss:0.70628\n",
      "[55]\tvalidation_0-logloss:0.70623\n",
      "[56]\tvalidation_0-logloss:0.70642\n",
      "[57]\tvalidation_0-logloss:0.70683\n",
      "[58]\tvalidation_0-logloss:0.70718\n",
      "[59]\tvalidation_0-logloss:0.70747\n",
      "[60]\tvalidation_0-logloss:0.70748\n",
      "[61]\tvalidation_0-logloss:0.70782\n",
      "[62]\tvalidation_0-logloss:0.70823\n",
      "[63]\tvalidation_0-logloss:0.70838\n",
      "[64]\tvalidation_0-logloss:0.70874\n",
      "[65]\tvalidation_0-logloss:0.70904\n",
      "[66]\tvalidation_0-logloss:0.70914\n",
      "[67]\tvalidation_0-logloss:0.70945\n",
      "[68]\tvalidation_0-logloss:0.70951\n",
      "[69]\tvalidation_0-logloss:0.70950\n",
      "[70]\tvalidation_0-logloss:0.70933\n",
      "[71]\tvalidation_0-logloss:0.70950\n",
      "[72]\tvalidation_0-logloss:0.70987\n",
      "[73]\tvalidation_0-logloss:0.71046\n",
      "[74]\tvalidation_0-logloss:0.71084\n",
      "[75]\tvalidation_0-logloss:0.71114\n",
      "[76]\tvalidation_0-logloss:0.71140\n",
      "[77]\tvalidation_0-logloss:0.71178\n",
      "[78]\tvalidation_0-logloss:0.71182\n",
      "[79]\tvalidation_0-logloss:0.71246\n",
      "[80]\tvalidation_0-logloss:0.71275\n",
      "[81]\tvalidation_0-logloss:0.71315\n",
      "[82]\tvalidation_0-logloss:0.71332\n",
      "[83]\tvalidation_0-logloss:0.71366\n",
      "[84]\tvalidation_0-logloss:0.71401\n",
      "[85]\tvalidation_0-logloss:0.71442\n",
      "[86]\tvalidation_0-logloss:0.71473\n",
      "[87]\tvalidation_0-logloss:0.71487\n",
      "[88]\tvalidation_0-logloss:0.71491\n",
      "[89]\tvalidation_0-logloss:0.71519\n",
      "[90]\tvalidation_0-logloss:0.71567\n",
      "[91]\tvalidation_0-logloss:0.71591\n",
      "[92]\tvalidation_0-logloss:0.71617\n",
      "[93]\tvalidation_0-logloss:0.71630\n",
      "[94]\tvalidation_0-logloss:0.71638\n",
      "[95]\tvalidation_0-logloss:0.71641\n",
      "[96]\tvalidation_0-logloss:0.71649\n",
      "[97]\tvalidation_0-logloss:0.71706\n",
      "[98]\tvalidation_0-logloss:0.71729\n",
      "[99]\tvalidation_0-logloss:0.71739\n",
      "Training accuracy:  0.6593888888888889\n",
      "Gradient Boosting Accuracy:  0.5081666666666667\n"
     ]
    }
   ],
   "source": [
    "# A skeleton for running your training dataframe through a SKLearn gradient boosting model. \n",
    "# You can also use other ML frameworks such as Pytorch, XGBoost, etc\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create the Gradient Boosting classifier\n",
    "xgb_model = xgb.XGBClassifier(random_state=93429834)\n",
    "\n",
    "# Train the model. Define X_train and y_train from your training dataframe using \n",
    "# sklearns train_test_split method\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "y1 = xgb_model.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y1)\n",
    "print(\"Training accuracy: \", accuracy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gb = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the Gradient Boosting model\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(\"Gradient Boosting Accuracy: \", accuracy_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.6563888888888889\n",
      "Gradient Boosting Accuracy:  0.5218333333333334\n"
     ]
    }
   ],
   "source": [
    "# A skeleton for running your training dataframe through a SKLearn gradient boosting model. \n",
    "# You can also use other ML frameworks such as Pytorch, XGBoost, etc\n",
    "\n",
    "# Create the Gradient Boosting classifier\n",
    "#gb_model = GradientBoostingClassifier(random_state=42, subsample=0.8, n_estimators=100, max_depth=11, loss='exponential', min_samples_leaf=2, learning_rate=0.001)\n",
    "gb_model = GradientBoostingClassifier(random_state=42, subsample=0.8, n_estimators=100, max_depth=11, loss='log_loss', min_samples_leaf=2, learning_rate=0.001)\n",
    "\n",
    "# Train the model. Define X_train and y_train from your training dataframe using \n",
    "# sklearns train_test_split method\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "y1 = gb_model.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y1)\n",
    "print(\"Training accuracy: \", accuracy)\n",
    "\n",
    "# Calculate the accuracy of the Gradient Boosting model\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(\"Gradient Boosting Accuracy: \", accuracy_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [Qiskit 1.2.0]",
   "language": "python",
   "name": "python3_qiskit_9vrlwn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
