{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Using environment Qiskit 1.2.0\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries (install in your environment first)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log2, sqrt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from scipy.stats import chi2, binom\n",
    "from scipy.fft import fft\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from your datafile or the provided datafile in this folder to classify QRNG data. \n",
    "# Suggested classification strategies include QRNG vs PRNG, QPU vs Simulator, or by individual QPU\n",
    "\n",
    "#Sample datafile of QRNG (IBM QPUs) vs PRNG data, 12k lines each. Label 1 is QRNG, label 2 is PRNG\n",
    "\n",
    "# Read data file, make dataframe, process labels, and combine/concatenate individual input lines into\n",
    "# larger input lines to create training and testing datasets to input into gradient boosting model.\n",
    "# Hint: use train_test_split method from sklearn\n",
    "\n",
    "def read_file_in_chunks(filename, N):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = file.read().strip()  # Read the entire file into a single string and remove any trailing spaces or newlines\n",
    "    return [data[i:i+N] for i in range(0, len(data), N)]  # Split the string into chunks of size N\n",
    "\n",
    "# Read from RNG1 and RNG2\n",
    "N = 1000\n",
    "rng1_bitstrings = read_file_in_chunks('RNG1-2.txt', N)\n",
    "rng2_bitstrings = read_file_in_chunks('RNG2-2.txt', N)\n",
    "\n",
    "# Create DataFrames for RNG1 and RNG2, and assign labels\n",
    "df_rng1 = pd.DataFrame({\n",
    "    'bitstrings': rng1_bitstrings,\n",
    "    'label': 1  # Label all RNG1 strings as '1'\n",
    "})\n",
    "\n",
    "df_rng2 = pd.DataFrame({\n",
    "    'bitstrings': rng2_bitstrings,\n",
    "    'label': 0  # Label all RNG2 strings as '2'\n",
    "})\n",
    "\n",
    "# Combine both DataFrames\n",
    "df = pd.concat([df_rng1, df_rng2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame (assuming you already have it)\n",
    "# df['bitstrings'] contains the bitstrings of length 100\n",
    "# df = pd.DataFrame({'bitstrings': ['1100100100001111110110...', '1010101010101010101010...', ...]})\n",
    "\n",
    "# Function to calculate Shannon entropy\n",
    "def shannon_entropy(bitstring):\n",
    "    # Convert bitstring to a numpy array of integers\n",
    "    bits = np.array([int(bit) for bit in bitstring])\n",
    "    \n",
    "    # Calculate the frequency of 0's and 1's\n",
    "    counts = np.bincount(bits)\n",
    "    probabilities = counts / len(bits)\n",
    "    \n",
    "    # Filter out zero probabilities to avoid log2(0)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Apply the entropy function to each bitstring in the DataFrame\n",
    "df['entropy'] = df['bitstrings'].apply(shannon_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_test(bits):\n",
    "        counts = np.bincount(list(bits), minlength=2)\n",
    "        observed = counts\n",
    "        expected = np.array([len(bits)/2, len(bits)/2])\n",
    "        chi_sq = np.sum((observed - expected) ** 2 / expected)\n",
    "        # Degrees of freedom = number of categories - 1 = 1\n",
    "        p_value = 1 - chi2.cdf(chi_sq, df=1)\n",
    "        return chi_sq, p_value\n",
    "\n",
    "# Apply the function and store results in two new columns\n",
    "df[['chi_sq', 'chi_sq_p_value']] = pd.DataFrame(df['bitstrings'].apply(chi_squared_test).tolist(), index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get the top 5 dominant magnitudes of frequencies\n",
    "def dominant_frequencies(bitstring, top_n=5):\n",
    "    # Convert bitstring to list of integers (0s and 1s)\n",
    "    bits = np.array([int(bit) for bit in bitstring])\n",
    "    n = len(bits)  # Length of the bitstring\n",
    "\n",
    "    # Convert bits to -1 and 1 for FFT\n",
    "    signal = 2 * bits - 1\n",
    "    fft_result = fft(signal)\n",
    "    \n",
    "    # Only take the positive frequencies\n",
    "    freqs = np.fft.fftfreq(n)\n",
    "    magnitudes = np.abs(fft_result)\n",
    "    positive_freqs = freqs[:n//2]\n",
    "    positive_magnitudes = magnitudes[:n//2]\n",
    "    \n",
    "    # Find the top_n dominant frequencies\n",
    "    indices = np.argsort(positive_magnitudes)[-top_n:]\n",
    "    dominant_mags = positive_magnitudes[indices]\n",
    "\n",
    "    # Sort the dominant magnitudes in descending order\n",
    "    sorted_mags = np.sort(dominant_mags)[::-1]\n",
    "    \n",
    "    # If fewer than top_n magnitudes, pad with NaNs\n",
    "    if len(sorted_mags) < top_n:\n",
    "        sorted_mags = np.pad(sorted_mags, (0, top_n - len(sorted_mags)), constant_values=np.nan)\n",
    "    \n",
    "    return sorted_mags\n",
    "\n",
    "# Apply the function and expand it into separate columns\n",
    "df[['freq_mag_1', 'freq_mag_2', 'freq_mag_3', 'freq_mag_4', 'freq_mag_5']] = pd.DataFrame(\n",
    "    df['bitstrings'].apply(dominant_frequencies).tolist(), index=df.index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(bitstring, lag=1):\n",
    "    # Convert the bitstring to a numpy array of integers (0 and 1)\n",
    "    bits = np.array([int(bit) for bit in bitstring])\n",
    "    \n",
    "    if lag >= len(bits):\n",
    "        raise ValueError(\"Lag is too large for the bitstream length.\")\n",
    "    \n",
    "    # Shift the bits by the given lag\n",
    "    shifted = np.roll(bits, -lag)\n",
    "    \n",
    "    # Calculate correlation excluding the wrapped-around elements\n",
    "    valid_length = len(bits) - lag\n",
    "    correlation = np.corrcoef(bits[:valid_length], shifted[:valid_length])[0, 1]\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "df['autocorrelation_lag1'] = df['bitstrings'].apply(autocorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_test(bitstring):\n",
    "    \"\"\"\n",
    "    Perform the Frequency Test on a bitstring.\n",
    "    \n",
    "    :param bitstring: A 1D numpy array or list of bits (0s and 1s).\n",
    "    :return: A tuple containing the number of 1s, number of 0s, and the test statistic.\n",
    "    \"\"\"\n",
    "    count_ones = list(bitstring).count('1')\n",
    "    count_zeros = len(bitstring) - count_ones\n",
    "    \n",
    "    return count_ones, count_zeros\n",
    "\n",
    "def block_frequency_test(bitstring, M):\n",
    "    \"\"\"\n",
    "    Perform the Block Frequency Test on a bitstring.\n",
    "    \n",
    "    :param bitstring: A 1D numpy array or list of bits (0s and 1s).\n",
    "    :param M: Block size for the block frequency test.\n",
    "    :return: The frequencies of 1s in each block and the test statistic.\n",
    "    \"\"\"\n",
    "    bitstring = [int(c) for c in bitstring]\n",
    "    n = len(bitstring)\n",
    "    num_blocks = n // M\n",
    "    block_frequencies = np.zeros(num_blocks)\n",
    "\n",
    "    # Compute block frequencies\n",
    "    for i in range(num_blocks):\n",
    "        block = bitstring[i * M:(i + 1) * M]\n",
    "        block_frequencies[i] = np.sum(block)\n",
    "\n",
    "    # Calculate the mean and variance of the block frequencies\n",
    "    mean_frequency = np.mean(block_frequencies)\n",
    "    variance_frequency = np.var(block_frequencies)\n",
    "\n",
    "    # The expected mean and variance for a random sequence\n",
    "    expected_mean = M / 2\n",
    "    expected_variance = M / 4\n",
    "\n",
    "    # Chi-squared statistic\n",
    "    chi_squared = ((mean_frequency - expected_mean) ** 2 / expected_variance) + \\\n",
    "                  (variance_frequency / expected_variance)\n",
    "\n",
    "    return block_frequencies.max(), block_frequencies.mean(), chi_squared\n",
    "\n",
    "df['freq_ones'], df['freq_zeros'] = zip(*df['bitstrings'].apply(frequency_test))\n",
    "\n",
    "# Set block size for Block Frequency Test\n",
    "M = 10  # Example block size\n",
    "df['block_freqs_max'], df['block_freqs_mean'], df['block_freq_chi_squared'] = zip(*df['bitstrings'].apply(lambda x: block_frequency_test(x, M)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runs_test(bits):\n",
    "    \"\"\"\n",
    "    Performs the Runs Test on a binary sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    bits (array-like): A binary sequence (1s and 0s).\n",
    "    \n",
    "    Returns:\n",
    "    (int, float): The number of runs and the p-value of the test.\n",
    "    \"\"\"\n",
    "    # Convert bits to a numpy array\n",
    "    bits = np.array([int(c) for c in bits])\n",
    "    \n",
    "    # Count the number of runs\n",
    "    runs = 1  # Start with the first run\n",
    "    for i in range(1, len(bits)):\n",
    "        if bits[i] != bits[i-1]:\n",
    "            runs += 1\n",
    "            \n",
    "    n1 = np.sum(bits)  # Number of 1s\n",
    "    n0 = len(bits) - n1  # Number of 0s\n",
    "    \n",
    "    # Calculate the expected number of runs and variance\n",
    "    expected_runs = (2 * n1 * n0) / (n1 + n0) + 1\n",
    "    variance_runs = (2 * n1 * n0 * (2 * n1 * n0 - n1 - n0)) / ((n1 + n0) ** 2 * (n1 + n0 - 1))\n",
    "    \n",
    "    # Z-score\n",
    "    z = (runs - expected_runs) / np.sqrt(variance_runs)\n",
    "    \n",
    "    # Calculate p-value from z-score\n",
    "    p_value = 1 - binom.cdf(runs, n=n1 + n0, p=0.5)  # Note: binom.cdf is not directly usable for z-scores\n",
    "    \n",
    "    return runs, p_value\n",
    "\n",
    "df['runs'], df['runs_p_value'] = zip(*df['bitstrings'].apply(runs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor(a, b):\n",
    "    \"\"\"\n",
    "    Returns the XOR of two binary values (0 or 1).\n",
    "    \n",
    "    Parameters:\n",
    "    a (int): First binary value (0 or 1).\n",
    "    b (int): Second binary value (0 or 1).\n",
    "    \n",
    "    Returns:\n",
    "    int: Result of a XOR b (0 or 1).\n",
    "    \"\"\"\n",
    "    return (a or b) and not (a and b)\n",
    "\n",
    "def linear_complexity(bits):\n",
    "    \"\"\"\n",
    "    Computes the Linear Complexity of a binary sequence using the Berlekamp-Massey algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    bits (array-like): A binary sequence (1s and 0s).\n",
    "    \n",
    "    Returns:\n",
    "    int: The linear complexity of the sequence.\n",
    "    \"\"\"\n",
    "    bits = np.array([int(c) for c in bits])\n",
    "    n = len(bits)\n",
    "    l = 0  # Linear complexity\n",
    "    m = -1  # Previous index where the error occurred\n",
    "    C = np.zeros(n)\n",
    "    B = np.zeros(n)\n",
    "    C[0] = 1  # The polynomial is initialized with a leading coefficient of 1\n",
    "    B[0] = 1\n",
    "    \n",
    "    for n in range(n):\n",
    "        # Calculate the discrepancy\n",
    "        discrepancy = bits[n]\n",
    "        for i in range(1, l + 1):\n",
    "            discrepancy = xor(discrepancy, (C[i] * bits[n - i]))\n",
    "        \n",
    "        if discrepancy == 1:  # An error occurred\n",
    "            T = C.copy()\n",
    "            for i in range(n - m):\n",
    "                if m + i < n:\n",
    "                    C[m + i] = xor(C[m + i], B[i])\n",
    "            if l <= n // 2:\n",
    "                l = n + 1 - l\n",
    "                m = n\n",
    "                B = T.copy()\n",
    "    \n",
    "    return l\n",
    "\n",
    "df['linear_complexity'] = df['bitstrings'].apply(linear_complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label                     1.000000\n",
      "chi_sq                    0.195938\n",
      "entropy                  -0.195914\n",
      "chi_sq_p_value           -0.152670\n",
      "block_freqs_mean          0.105297\n",
      "freq_zeros               -0.105297\n",
      "freq_ones                 0.105297\n",
      "block_freq_chi_squared    0.067186\n",
      "freq_mag_4                0.020589\n",
      "block_freqs_max           0.017653\n",
      "freq_mag_5                0.017355\n",
      "runs_p_value              0.017164\n",
      "runs                     -0.014431\n",
      "freq_mag_2                0.012462\n",
      "freq_mag_3                0.012420\n",
      "freq_mag_1                0.010301\n",
      "linear_complexity        -0.003191\n",
      "autocorrelation_lag1      0.002393\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Normalize a specific column in the DataFrame (let's assume 'column_name' is the one you want to normalize)\n",
    "def zscore_col(df, colname):\n",
    "    df[colname] = (df[colname] - df[colname].mean()) / df[colname].std()\n",
    "\n",
    "#metrics = ['linear_complexity', 'runs_p_value', 'runs', 'chi_sq','chi_sq_p_value','entropy', 'freq_ones', 'freq_zeros', 'block_freqs_max', 'block_freqs_mean', 'block_freq_chi_squared']\n",
    "metrics = ['freq_mag_1', 'freq_mag_2', 'freq_mag_3', 'freq_mag_4', 'freq_mag_5', 'autocorrelation_lag1', 'linear_complexity', 'runs_p_value', 'runs', 'chi_sq','chi_sq_p_value','entropy', 'freq_ones', 'freq_zeros', 'block_freqs_max', 'block_freqs_mean', 'block_freq_chi_squared']\n",
    "\n",
    "\n",
    "\n",
    "for colname in metrics:\n",
    "    zscore_col(df, colname)\n",
    "\n",
    "# Calculate correlation\n",
    "correlation_matrix = df[metrics + ['label']].corr()\n",
    "correlation_with_target = correlation_matrix['label'].sort_values(ascending=False, key=abs)\n",
    "print(correlation_with_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'strings' column into separate columns for each character\n",
    "df_split = df['bitstrings'].str.split('', expand=True)\n",
    "\n",
    "# Fill NaN values with an empty string (optional)\n",
    "df_split.fillna('', inplace=True)\n",
    "\n",
    "# Concatenate the split DataFrame with the original DataFrame\n",
    "df_combined = pd.concat([df, df_split], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11331242  0.70308799  1.38687271 ...  0.3937202  -0.15626922\n",
      "  -2.76190975]\n",
      " [ 2.14834186  1.5427716  -0.15725071 ...  0.3937202   2.16616504\n",
      "  -0.82426788]\n",
      " [-0.18943043 -0.67502495 -0.40451501 ... -1.23546684 -0.53433991\n",
      "  -2.07628263]\n",
      " ...\n",
      " [-1.34594285 -0.91642668 -0.38062398 ...  0.3937202  -1.99261259\n",
      "  -0.85407776]\n",
      " [-1.16045472 -0.56543332 -0.95388493 ...  0.3937202  -1.2364712\n",
      "  -1.15217651]\n",
      " [-0.91257528 -0.51149325 -0.9529761  ...  0.3937202  -1.07444091\n",
      "   0.90470486]]\n",
      "[0 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Prepare input features (X) and target labels (y)\n",
    "X = pd.DataFrame(df[metrics])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(df['label'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Hyperparameters: {'subsample': 1.0, 'n_estimators': 100, 'min_samples_split': 9, 'min_samples_leaf': 9, 'max_features': 'log2', 'max_depth': 3, 'learning_rate': 0.001}\n",
      "Accuracy: 0.5877777777777777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.89      0.68       903\n",
      "           1       0.72      0.28      0.41       897\n",
      "\n",
      "    accuracy                           0.59      1800\n",
      "   macro avg       0.64      0.59      0.54      1800\n",
      "weighted avg       0.64      0.59      0.55      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tune_gradient_boosting(X_train, y_train, n_iter=100, random_state=42):\n",
    "    # Define the parameter grid\n",
    "    param_dist = {\n",
    "        'n_estimators': np.arange(50, 300, 50),  # Number of boosting stages\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],  # Learning rate\n",
    "        'max_depth': np.arange(3, 11),  # Maximum depth of the individual regression estimators\n",
    "        'min_samples_split': np.arange(2, 11),  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': np.arange(1, 11),  # Minimum number of samples required to be at a leaf node\n",
    "        'subsample': [0.5, 0.7, 1.0],  # Fraction of samples to be used for fitting the individual base learners\n",
    "        'max_features': [None, 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "    }\n",
    "\n",
    "    # Create the Gradient Boosting Classifier\n",
    "    gb_classifier = GradientBoostingClassifier(random_state=random_state)\n",
    "\n",
    "    # Setup RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=gb_classifier,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        scoring='accuracy',\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        verbose=1,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and model\n",
    "    best_params = random_search.best_params_\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    return best_params, best_model\n",
    "\n",
    "# Tune the Gradient Boosting model\n",
    "best_params, best_model = tune_gradient_boosting(X_train, y_train)\n",
    "\n",
    "    # Output the best parameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Make predictions with the best model\n",
    "y_pred = best_model.predict(X_train)\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.69314\n",
      "[1]\tvalidation_0-logloss:0.69310\n",
      "[2]\tvalidation_0-logloss:0.69306\n",
      "[3]\tvalidation_0-logloss:0.69302\n",
      "[4]\tvalidation_0-logloss:0.69298\n",
      "[5]\tvalidation_0-logloss:0.69294\n",
      "[6]\tvalidation_0-logloss:0.69290\n",
      "[7]\tvalidation_0-logloss:0.69286\n",
      "[8]\tvalidation_0-logloss:0.69281\n",
      "[9]\tvalidation_0-logloss:0.69277\n",
      "[10]\tvalidation_0-logloss:0.69272\n",
      "[11]\tvalidation_0-logloss:0.69268\n",
      "[12]\tvalidation_0-logloss:0.69264\n",
      "[13]\tvalidation_0-logloss:0.69260\n",
      "[14]\tvalidation_0-logloss:0.69256\n",
      "[15]\tvalidation_0-logloss:0.69252\n",
      "[16]\tvalidation_0-logloss:0.69247\n",
      "[17]\tvalidation_0-logloss:0.69243\n",
      "[18]\tvalidation_0-logloss:0.69239\n",
      "[19]\tvalidation_0-logloss:0.69235\n",
      "[20]\tvalidation_0-logloss:0.69231\n",
      "[21]\tvalidation_0-logloss:0.69227\n",
      "[22]\tvalidation_0-logloss:0.69222\n",
      "[23]\tvalidation_0-logloss:0.69218\n",
      "[24]\tvalidation_0-logloss:0.69214\n",
      "[25]\tvalidation_0-logloss:0.69210\n",
      "[26]\tvalidation_0-logloss:0.69206\n",
      "[27]\tvalidation_0-logloss:0.69202\n",
      "[28]\tvalidation_0-logloss:0.69198\n",
      "[29]\tvalidation_0-logloss:0.69194\n",
      "[30]\tvalidation_0-logloss:0.69190\n",
      "[31]\tvalidation_0-logloss:0.69186\n",
      "[32]\tvalidation_0-logloss:0.69181\n",
      "[33]\tvalidation_0-logloss:0.69178\n",
      "[34]\tvalidation_0-logloss:0.69174\n",
      "[35]\tvalidation_0-logloss:0.69170\n",
      "[36]\tvalidation_0-logloss:0.69166\n",
      "[37]\tvalidation_0-logloss:0.69162\n",
      "[38]\tvalidation_0-logloss:0.69158\n",
      "[39]\tvalidation_0-logloss:0.69154\n",
      "[40]\tvalidation_0-logloss:0.69150\n",
      "[41]\tvalidation_0-logloss:0.69146\n",
      "[42]\tvalidation_0-logloss:0.69142\n",
      "[43]\tvalidation_0-logloss:0.69138\n",
      "[44]\tvalidation_0-logloss:0.69134\n",
      "[45]\tvalidation_0-logloss:0.69130\n",
      "[46]\tvalidation_0-logloss:0.69127\n",
      "[47]\tvalidation_0-logloss:0.69122\n",
      "[48]\tvalidation_0-logloss:0.69119\n",
      "[49]\tvalidation_0-logloss:0.69115\n",
      "[50]\tvalidation_0-logloss:0.69111\n",
      "[51]\tvalidation_0-logloss:0.69107\n",
      "[52]\tvalidation_0-logloss:0.69103\n",
      "[53]\tvalidation_0-logloss:0.69099\n",
      "[54]\tvalidation_0-logloss:0.69096\n",
      "[55]\tvalidation_0-logloss:0.69092\n",
      "[56]\tvalidation_0-logloss:0.69088\n",
      "[57]\tvalidation_0-logloss:0.69084\n",
      "[58]\tvalidation_0-logloss:0.69080\n",
      "[59]\tvalidation_0-logloss:0.69076\n",
      "[60]\tvalidation_0-logloss:0.69073\n",
      "[61]\tvalidation_0-logloss:0.69069\n",
      "[62]\tvalidation_0-logloss:0.69065\n",
      "[63]\tvalidation_0-logloss:0.69061\n",
      "[64]\tvalidation_0-logloss:0.69058\n",
      "[65]\tvalidation_0-logloss:0.69054\n",
      "[66]\tvalidation_0-logloss:0.69050\n",
      "[67]\tvalidation_0-logloss:0.69046\n",
      "[68]\tvalidation_0-logloss:0.69043\n",
      "[69]\tvalidation_0-logloss:0.69039\n",
      "[70]\tvalidation_0-logloss:0.69035\n",
      "[71]\tvalidation_0-logloss:0.69031\n",
      "[72]\tvalidation_0-logloss:0.69028\n",
      "[73]\tvalidation_0-logloss:0.69024\n",
      "[74]\tvalidation_0-logloss:0.69020\n",
      "[75]\tvalidation_0-logloss:0.69017\n",
      "[76]\tvalidation_0-logloss:0.69013\n",
      "[77]\tvalidation_0-logloss:0.69009\n",
      "[78]\tvalidation_0-logloss:0.69006\n",
      "[79]\tvalidation_0-logloss:0.69002\n",
      "[80]\tvalidation_0-logloss:0.68998\n",
      "[81]\tvalidation_0-logloss:0.68995\n",
      "[82]\tvalidation_0-logloss:0.68991\n",
      "[83]\tvalidation_0-logloss:0.68987\n",
      "[84]\tvalidation_0-logloss:0.68984\n",
      "[85]\tvalidation_0-logloss:0.68980\n",
      "[86]\tvalidation_0-logloss:0.68977\n",
      "[87]\tvalidation_0-logloss:0.68973\n",
      "[88]\tvalidation_0-logloss:0.68970\n",
      "[89]\tvalidation_0-logloss:0.68966\n",
      "[90]\tvalidation_0-logloss:0.68962\n",
      "[91]\tvalidation_0-logloss:0.68959\n",
      "[92]\tvalidation_0-logloss:0.68955\n",
      "[93]\tvalidation_0-logloss:0.68952\n",
      "[94]\tvalidation_0-logloss:0.68948\n",
      "[95]\tvalidation_0-logloss:0.68944\n",
      "[96]\tvalidation_0-logloss:0.68941\n",
      "[97]\tvalidation_0-logloss:0.68937\n",
      "[98]\tvalidation_0-logloss:0.68934\n",
      "[99]\tvalidation_0-logloss:0.68930\n",
      "Training accuracy:  0.5922222222222222\n",
      "Gradient Boosting Accuracy for XGB:  0.5666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.qbraid/environments/qiskit_9vrlwn/pyenv/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [23:15:58] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"max_features\", \"min_samples_leaf\", \"min_samples_split\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# A skeleton for running your training dataframe through a SKLearn gradient boosting model. \n",
    "# You can also use other ML frameworks such as Pytorch, XGBoost, etc\n",
    "# Create the Gradient Boosting classifier\n",
    "xgb_model = xgb.XGBClassifier(**best_params)\n",
    "\n",
    "# Train the model. Define X_train and y_train from your training dataframe using \n",
    "# sklearns train_test_split method\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "y1 = xgb_model.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y1)\n",
    "print(\"Training accuracy: \", accuracy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gb = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the Gradient Boosting model\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(\"Gradient Boosting Accuracy for XGB: \", accuracy_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy For SVM:  0.6016666666666667\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf', C=0.7)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y1 = clf.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y1)\n",
    "#print(\"Training accuracy: \", accuracy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gb = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the Gradient Boosting model\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Testing Accuracy For SVM: \", accuracy_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.595\n",
      "Gradient Boosting Accuracy for GB:  0.5683333333333334\n"
     ]
    }
   ],
   "source": [
    "# A skeleton for running your training dataframe through a SKLearn gradient boosting model. \n",
    "# You can also use other ML frameworks such as Pytorch, XGBoost, etc\n",
    "\n",
    "# Create the Gradient Boosting classifier\n",
    "#gb_model = GradientBoostingClassifier(random_state=42, subsample=0.8, n_estimators=100, max_depth=11, loss='log_loss', min_samples_leaf=2, learning_rate=0.001)\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    **best_params\n",
    ")\n",
    "# Train the model. Define X_train and y_train from your training dataframe using \n",
    "# sklearns train_test_split method\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "y1 = gb_model.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y1)\n",
    "print(\"Training accuracy: \", accuracy)\n",
    "\n",
    "# Calculate the accuracy of the Gradient Boosting model\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(\"Gradient Boosting Accuracy for GB: \", accuracy_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [Qiskit 1.2.0]",
   "language": "python",
   "name": "python3_qiskit_9vrlwn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
